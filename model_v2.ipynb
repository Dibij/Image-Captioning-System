{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.layers import (Layer, Dense, Embedding, MultiHeadAttention, LayerNormalization, Dropout, Input, Add, Concatenate)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration\n",
    "class Config:\n",
    "\n",
    "  #Dataset\n",
    "  Image_Dir = '/content/coco_dataset/images/train2017'\n",
    "  Ann_path = '/content/coco_dataset/annotations/annotations/captions_train2017.json'\n",
    "\n",
    "  EMBED_DIM = 512\n",
    "  TRANSFORMER_HEADS = 8\n",
    "  TRANSFORMER_LAYERS = 3\n",
    "  DROPOUT_RATE = 0.2\n",
    "  LEARNING_RATE = 0.0001\n",
    "  BATCH_SIZE = 64\n",
    "  EPOCHS = 30\n",
    "  MAX_SEQ_LENGTH = 40\n",
    "  VOCAB_SIZE = 10000\n",
    "\n",
    "  #Data Augmentation\n",
    "  ROTATION_RANGE = 15\n",
    "  WIDTH_SHIFT_RANGE = 0.1\n",
    "  BRIGHTNESS_RANGE = [0.9, 1.1]\n",
    "\n",
    "  MODEL_SAVE_PATH = 'coco_captioning_transformer.h5'\n",
    "  TOKENIZER_SAVE_PATH = 'tokenizer.pkl'\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f71cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Loading & Augmentation\n",
    "\n",
    "class CocoDataLoader:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.image_dir = config.Image_Dir\n",
    "        self.augmentor = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            rotation_range=config.ROTATION_RANGE,\n",
    "            width_shift_range=config.WIDTH_SHIFT_RANGE,\n",
    "            brightness_range=config.BRIGHTNESS_RANGE,\n",
    "            horizontal_flip=True\n",
    "        )\n",
    "\n",
    "    def load_data(self, max_images=2000):\n",
    "        with open(self.config.Ann_path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "            # Extract images and captions (up to max_images)\n",
    "            images = []\n",
    "            captions = []\n",
    "\n",
    "            for ann in data['annotations']:\n",
    "                if len(images) >= max_images:\n",
    "                    break  # Stop after collecting 2000 images\n",
    "\n",
    "                img_path = os.path.join(self.image_dir, f\"{ann['image_id']:012d}.jpg\")\n",
    "                if os.path.exists(img_path):\n",
    "                    images.append(img_path)\n",
    "                    captions.append(ann['caption'])\n",
    "\n",
    "            return images, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Extraction\n",
    "class FeatureExtractor:\n",
    "  def __init__(self):\n",
    "    self.model = EfficientNetB4(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "  def extract(self, image_path):\n",
    "    try:\n",
    "      img = Image.open(image_path).convert('RGB').resize((380, 380))\n",
    "      img = np.array(img) / 255.0\n",
    "      img = tf.keras.applications.efficientnet.preprocess_input(img)\n",
    "      features = self.model.predict(np.expand_dims(img, axis = 0), verbose=0)\n",
    "      return features.flatten()\n",
    "    except Exception as e:\n",
    "      print(f\"Error extracting features from image {image_path}: {e}\")\n",
    "      return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916248e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Text Processing\n",
    "class TextProcessor:\n",
    "  def __init__(self, config):\n",
    "    self.config = config\n",
    "    self.tokenizer = Tokenizer(num_words=config.VOCAB_SIZE, oov_token= '<unk>')\n",
    "  def preprocess(self, text):\n",
    "    text = text.lower()\n",
    "    text  =re.sub(r'[^\\w\\s]', '', text)\n",
    "    return ' '.join(word_tokenize(text))\n",
    "  def build_vocab(self, captions):\n",
    "    processed = [self.preprocess(c) for c in captions]\n",
    "    self.tokenizer.fit_on_texts(processed)\n",
    "    self.tokenizer.word_index['<start>'] = len(self.tokenizer.word_index) + 1\n",
    "    self.tokenizer.word_index['<end>'] = len(self.tokenizer.word_index) + 1\n",
    "    return self.tokenizer\n",
    "  def encode(self, captions):\n",
    "    sequences =[]\n",
    "    for cap in captions:\n",
    "      seq = self.tokenizer.texts_to_sequences(\n",
    "          ['<start>' + self.preprocess(cap) + ' <end>']\n",
    "      )[0]\n",
    "      sequences.append(seq)\n",
    "      return pad_sequences(sequences, maxlen=self.config.MAX_SEQ_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf0166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Transformer Architecture\n",
    "class TransformerBlock(Layer):\n",
    "  def __init__(self, embed_dim, num_heads, dropout_rate=0.2):\n",
    "    super().__init()\n",
    "    self.att = MultiHeadAttention(num_heads, embed_dim)\n",
    "    self.fin = tf.keras.Sequential([\n",
    "        Dense(embed_dim, activation ='relu'),\n",
    "        Dense(embed_dim)\n",
    "    ])\n",
    "    self.layernorm1 = LayerNormalization()\n",
    "    self.layernorm2 = LayerNormalization()\n",
    "    self.dropout1 = Dropout(dropout_rate)\n",
    "    self.dropout2 = Dropout(dropout_rate)\n",
    "  def call(self, inputs):\n",
    "    attn_output = self.att(inputs, inputs)\n",
    "    attn_output = self.dropout1(attn_output)\n",
    "    out1 = self.layernorm1(inputs + attn_output)\n",
    "    ffn_output = self.fin(out1)\n",
    "    return self.layernorm2(out1 + self.dropout2(ffn_output))\n",
    "class ImageCaptioningModel:\n",
    "  def __init__(self, config):\n",
    "    self.config = config\n",
    "\n",
    "  def build(self):\n",
    "    image_input = Input(shape=(1792, ))\n",
    "    image_dense = Dense(self.config.EMBED_DIM, activation='relu')(image_input)\n",
    "    image_features = tf.expand_dims(image_dense, 1)\n",
    "\n",
    "    caption_input = Input(shape=(self.config.MAX_SEQ_LENGTH, ))\n",
    "    embedding = Embedding(\n",
    "        self.config.VOCAB_SIZE.\n",
    "        self.config.EMBED_DIM,\n",
    "        mask_zero=True\n",
    "    )(caption_input)\n",
    "\n",
    "    x = embedding\n",
    "    for _ in range(self.config.TRANSFORMER_LAYERS):\n",
    "      x = TransformerBlock(\n",
    "          self.config.EMBED_DIM,\n",
    "          self.config.TRANSFORMER_HEADS,\n",
    "          self.config.DROPOUT_RATE\n",
    "      )(x)\n",
    "    outputs = Dense(self.config.VOCAB_SIZE, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[image_input,  caption_input], outputs = outputs)\n",
    "    model.compile(\n",
    "        optimizer = Adam(self.config.LEARNING_RATE),\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e202dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Training Pipeline\n",
    "\n",
    "def train():\n",
    "  loader = CocoDataLoader(config)\n",
    "  image_paths, captions = loader.load_data()\n",
    "\n",
    "\n",
    "  fe = FeatureExtractor()\n",
    "  features = []\n",
    "  valid_captions = []\n",
    "  for img_path, cap in zip(image_paths, captions):\n",
    "    feat = fe.extract(img_path)\n",
    "    if feat is not None:\n",
    "      features.append(feat)\n",
    "      valid_captions.append(cap)\n",
    "\n",
    "  tp = TextProcessor(config)\n",
    "  tokenizer = tp.build_vocab(valid_captions)\n",
    "  sequences = tp.encode(valid_captions)\n",
    "\n",
    "  X_train, X_val, y_train, y_val = train_test_split(\n",
    "      np.array(features),\n",
    "      np.array(sequences),\n",
    "      test_size = 0.2,\n",
    "      random_state = 42\n",
    "  )\n",
    "\n",
    "  model = ImageCaptioningModel(config).build()\n",
    "\n",
    "\n",
    "  callbacks =[\n",
    "      EarlyStopping(patience=5, restore_best_weights=True),\n",
    "      LearningRateScheduler(lambda epoch: config.LEARNING_RATE * (0.9 ** epoch))\n",
    "  ]\n",
    "\n",
    "  history = model.fit(\n",
    "      x=[X_train, y_train[:, :-1]],\n",
    "      y=y_train[:, 1:],\n",
    "      validation_data=([X_val, y_val[:, :-1]], y_val[:, 1:]),\n",
    "      epochs=config.EPOCHS,\n",
    "      batch_size=config.BATCH_SIZE,\n",
    "      callbacks=callbacks\n",
    "  )\n",
    "\n",
    "  model.save(config.MODEL_SAVE_PATH)\n",
    "  with open(config.TOKENIZER_SAVE_PATH, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "\n",
    "  return model, tokenizer, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f8f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Evaluation & Inference\n",
    "class CaptionGenerator:\n",
    "  def __init__(self, model_path, tokenizer_path):\n",
    "    self.model = tf.keras.model.load_model(\n",
    "        model_path,\n",
    "        custom_objects={'TransformerBlock': TransformerBlock}\n",
    "    )\n",
    "    with open (tokenizer_path, 'rb') as f:\n",
    "      self.tokenizer = pickle.load(f)\n",
    "\n",
    "  def generate_caption(self, image_path, beam_width=3):\n",
    "    fe = FeatureExtractor()\n",
    "    features = fe.extractor(image_path)\n",
    "    if features is None:\n",
    "      return None\n",
    "\n",
    "    start_token = self.tokenizer.word_index['<start>']\n",
    "    end_token = self.tokenizer.word_index['<end>']\n",
    "\n",
    "    sequences = [[[start_token], 0.0]]\n",
    "\n",
    "    for _ in range(config.MAX_SEQ_LENGTH -  1):\n",
    "      all_candidates = []\n",
    "      for seq, score in sequences:\n",
    "        if seq[-1] == end_token:\n",
    "          all_candidates.append((seq, score))\n",
    "          continue\n",
    "\n",
    "        input_seq = pad_sequences([seq], max_len=config.MAX_SEQ_LENGTH, padding='post')\n",
    "\n",
    "        preds = self.model.predict(\n",
    "            [np.array([features]), input_seq],\n",
    "            verbose=0\n",
    "        )[0]\n",
    "\n",
    "        top_k = np.argsort(preds[-1])[-beam_width:]\n",
    "        for word_id in top_k:\n",
    "          candidate_seq = seq + [word_id]\n",
    "          candidate_score  =score = np.log(preds[-1][word_id])\n",
    "          all_candidates.append((candidate_seq, candidate_score))\n",
    "      ordered = sorted(all_candidates, key=lambda x: x[1])\n",
    "      sequences = ordered[:beam_width]\n",
    "    best_seq = sequences[0][0]\n",
    "    caption = []\n",
    "    for word_id in best_seq:\n",
    "      word = self.tokenizer.index_word.get(word_id, '<unk>')\n",
    "      if word == '<end>':\n",
    "        break\n",
    "      if word != '<start>':\n",
    "        caption.append(word)\n",
    "    return ' '.join(caption)\n",
    "\n",
    "  def evaluate(self, test_images, test_captions):\n",
    "    refrences = []\n",
    "    hypothesis = []\n",
    "\n",
    "    for img_path, true_cap in zip (test_images, test_captions):\n",
    "      pred_cap = self.generate_caption(img_path)\n",
    "      if pred_cap:\n",
    "        refrences.append([true_cap.split()])\n",
    "        hypothesis.append(pred_cap.split())\n",
    "\n",
    "    bleu4 = corpus_bleu(refrences, hypothesis)\n",
    "    return bleu4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9756e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Main\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  model, tokenizer, history = train()\n",
    "\n",
    "  generator = CaptionGenerator(config.MODEL_SAVE_PATH, config.TOKENIZER_SAVE_PATH)\n",
    "\n",
    "  test_image = \"/content/coco_dataset/images/val2017/000000000139.jpg\"\n",
    "  print(\"Generated Caption: \", generator.generate_caption(test_image))\n",
    "\n",
    "  sample_images = [...]  # List of test image paths\n",
    "  sample_captions = [...]  # List of ground truth captions\n",
    "  bleu_score = generator.evaluate(sample_images, sample_captions)\n",
    "  print(f\"BLEU-4 Score: {bleu_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
