{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Layer, Dense, Embedding, LSTM, Input, Concatenate, Lambda, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 1. LOAD AND PREPARE DATA\n",
    "# ======================\n",
    "with open('/content/coco_dataset/annotations/annotations/captions_train2017.json', 'r') as f:\n",
    "    captions_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 2. FEATURE EXTRACTION\n",
    "# ======================\n",
    "print(\"Loading InceptionV3 model...\")\n",
    "inception_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "def extract_features(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        img = img.resize((299, 299))\n",
    "        img = np.array(img) / 255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = preprocess_input(img)\n",
    "        features = inception_model.predict(img, verbose=0)\n",
    "        return features.flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"Extracting image features...\")\n",
    "features_dict = {}\n",
    "for img_info in captions_data['images'][:1000]:  # Using first 1000 for demo\n",
    "    image_path = os.path.join('/content/coco_dataset/images/train2017', img_info['file_name'])\n",
    "    if os.path.exists(image_path):\n",
    "        features = extract_features(image_path)\n",
    "        if features is not None:\n",
    "            features_dict[img_info['id']] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 3. CAPTION PREPROCESSING\n",
    "# ======================\n",
    "def preprocess_caption(caption):\n",
    "    caption = caption.lower()\n",
    "    caption = re.sub(r'[^\\w\\s]', '', caption)\n",
    "    tokens = word_tokenize(caption)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"Preprocessing captions...\")\n",
    "all_captions = []\n",
    "image_ids = []\n",
    "for ann in captions_data['annotations']:\n",
    "    if ann['image_id'] in features_dict:\n",
    "        preprocessed_caption = preprocess_caption(ann['caption'])\n",
    "        all_captions.append(preprocessed_caption)\n",
    "        image_ids.append(ann['image_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 4. TOKENIZATION\n",
    "# ======================\n",
    "tokenizer = Tokenizer(oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "\n",
    "# Add special tokens\n",
    "tokenizer.word_index['<start>'] = len(tokenizer.word_index) + 1\n",
    "tokenizer.word_index['<end>'] = len(tokenizer.word_index) + 1\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 5. SEQUENCE CREATION\n",
    "# ======================\n",
    "max_length = max(len(seq) for seq in tokenizer.texts_to_sequences(all_captions)) \n",
    "print(f\"Maximum sequence length: {max_length}\")\n",
    "\n",
    "def encode_caption(caption):\n",
    "    caption = '<start> ' + caption + ' <end>'\n",
    "    seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "    return seq\n",
    "\n",
    "sequences = [encode_caption(cap) for cap in all_captions]\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 6. DATA PREPARATION\n",
    "# ======================\n",
    "image_features = []\n",
    "captions = []\n",
    "for img_id, seq in zip(image_ids, padded_sequences):\n",
    "    if img_id in features_dict:\n",
    "        image_features.append(features_dict[img_id])\n",
    "        captions.append(seq)\n",
    "\n",
    "image_features = np.array(image_features)\n",
    "captions = np.array(captions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 7. TRAIN-VAL SPLIT\n",
    "# ======================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    image_features, captions, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 8. ATTENTION MODEL\n",
    "# ======================\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(features) + self.W2(hidden_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "def build_attention_model(vocab_size, max_length, embedding_dim=256, units=256):\n",
    "    # Image encoder\n",
    "    image_input = Input(shape=(2048,))\n",
    "    image_dense = Dense(units, activation='relu')(image_input)\n",
    "    image_features = Reshape((1, units))(image_dense)\n",
    "    \n",
    "    # Caption decoder\n",
    "    caption_input = Input(shape=(max_length,))\n",
    "    embedding = Embedding(vocab_size, embedding_dim, mask_zero=True)(caption_input)\n",
    "    \n",
    "    # Initialize LSTM and attention\n",
    "    lstm = LSTM(units, return_sequences=True, return_state=True)\n",
    "    attention = BahdanauAttention(units)\n",
    "    \n",
    "    # Initial states\n",
    "    hidden = Dense(units)(image_dense)\n",
    "    cell = Dense(units)(image_dense)\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    # Process each time step\n",
    "    for t in range(max_length):\n",
    "        context_vector, _ = attention(image_features, hidden)\n",
    "        lstm_input = Concatenate()([embedding[:, t, :], context_vector])\n",
    "        lstm_input = Reshape((1, embedding_dim + units))(lstm_input)\n",
    "        \n",
    "        output, hidden, cell = lstm(lstm_input, initial_state=[hidden, cell])\n",
    "        output = Dense(vocab_size, activation='softmax')(output[:, -1, :])\n",
    "        outputs.append(output)\n",
    "    \n",
    "    outputs = Lambda(lambda x: tf.stack(x, axis=1))(outputs) \n",
    "    return Model(inputs=[image_input, caption_input], outputs=outputs)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_attention_model(vocab_size, max_length - 1)  \n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Verify shapes before training\n",
    "print(\"\\nShape verification:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train input shape: {y_train[:, :-1].shape}\")\n",
    "print(f\"y_train target shape: {y_train[:, 1:].shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 9. MODEL TRAINING\n",
    "# ======================\n",
    "history = model.fit(\n",
    "    x=[X_train, y_train[:, :-1]],  # Input: Image features, captions without last word\n",
    "    y=y_train[:, 1:],              # Target: Captions without first word\n",
    "    validation_data=([X_val, y_val[:, :-1]], y_val[:, 1:]),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 10. SAVE MODEL\n",
    "# ======================\n",
    "model.save('attention_image_captioning_model.h5')\n",
    "print(\"Model saved with attention mechanism!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 11. PREDICTION FUNCTION\n",
    "# ======================\n",
    "def predict_caption(image_path, model, tokenizer, max_length=None):\n",
    "    # Extract features\n",
    "    features = extract_features(image_path)\n",
    "    if features is None:\n",
    "        return \"Could not process image\"\n",
    "    \n",
    "    features = np.expand_dims(features, axis=0)\n",
    "    in_text = '<start>'\n",
    "    \n",
    "    # Get the model's expected sequence length from its input shape\n",
    "    model_sequence_length = model.input_shape[1][1]\n",
    "    \n",
    "    for _ in range(model_sequence_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # Pad to the model's expected length\n",
    "        sequence = pad_sequences([sequence], maxlen=model_sequence_length, padding='post')\n",
    "        yhat = model.predict([features, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat[0, -1, :])\n",
    "        word = tokenizer.index_word.get(yhat, '')\n",
    "        \n",
    "        if word == '<end>':\n",
    "            break\n",
    "            \n",
    "        in_text += ' ' + word\n",
    "    \n",
    "    return in_text.replace('<start>', '').strip()\n",
    "\n",
    "# Test prediction\n",
    "test_image = \"/content/coco_dataset/images/train2017/000000000009.jpg\"\n",
    "print(\"\\nGenerated caption:\", predict_caption(test_image, model, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
